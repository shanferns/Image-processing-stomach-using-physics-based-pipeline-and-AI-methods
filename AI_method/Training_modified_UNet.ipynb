{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4dd55-8486-48c0-aedd-13933ecae291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee0aadaf-bed2-497e-b1a6-f9d05c3d1d09",
   "metadata": {},
   "source": [
    "# Modified UNet Training and implementation\n",
    "\n",
    "## Key Organ-Specific Modifications Highlighted\n",
    "\n",
    "Loss function:\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.0]))\n",
    "\n",
    "\n",
    "## Penalizes missing organ voxels more than background\n",
    "\n",
    "Handles class imbalance common in organ segmentation\n",
    "\n",
    "BatchNorm included in all Conv blocks\n",
    "\n",
    "Stabilizes MRI intensity variations\n",
    "\n",
    "Helps faster convergence\n",
    "\n",
    "Smaller network depth\n",
    "\n",
    "Only 2 downsampling steps + bottleneck → enough for organ, reduces GPU memory usage\n",
    "\n",
    "Sigmoid applied at inference\n",
    "\n",
    "y = torch.sigmoid(y)\n",
    "y = (y > 0.5).float()\n",
    "\n",
    "\n",
    "Produces binary organ mask\n",
    "\n",
    "## Files used for training are 4D files\n",
    "\n",
    "Input -> input.nii (4D MRI file)\n",
    "\n",
    "Output -> output.nii (4D stomach organ segmented file)\n",
    "\n",
    "## Saved files\n",
    "\n",
    "- unet_trained.pth - This is the trained UNet and can be used for segmenting the stomach\n",
    "- predicted_segmentation.nii - This is a 4D file to show how well the UNet perfomed\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "372900e6-1ea7-4973-adda-48bbece557b3",
   "metadata": {},
   "source": [
    "INPUT: (T, 1, X, Y, Z)  →  MRI volume (grayscale)\n",
    "\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────┐\n",
    "│ ConvBlock1 (DoubleConv 1→32) │\n",
    "│ Conv3D + BatchNorm + ReLU ×2 │\n",
    "└─────────────────────┘\n",
    "        │\n",
    "MaxPool3D 2×2×2  (↓ spatial dims)\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────┐\n",
    "│ ConvBlock2 (DoubleConv 32→64) │\n",
    "│ Conv3D + BatchNorm + ReLU ×2 │\n",
    "└─────────────────────┘\n",
    "        │\n",
    "MaxPool3D 2×2×2\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────┐\n",
    "│ Bottleneck ConvBlock3 (64→128) │\n",
    "│ Conv3D + BatchNorm + ReLU ×2 │\n",
    "└─────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "UpConv3D 128→64, stride=2  (upsampling)\n",
    "        │\n",
    "        ▼\n",
    "Concatenate with ConvBlock2 output → 128 channels (Skip connection)\n",
    "        │\n",
    "        ▼\n",
    "DoubleConv 128→64\n",
    "        │\n",
    "        ▼\n",
    "UpConv3D 64→32, stride=2\n",
    "        │\n",
    "        ▼\n",
    "Concatenate with ConvBlock1 output → 64 channels (Skip connection)\n",
    "        │\n",
    "        ▼\n",
    "DoubleConv 64→32\n",
    "        │\n",
    "        ▼\n",
    "Final Conv3D 32→1 (kernel 1×1×1)\n",
    "        │\n",
    "        ▼\n",
    "OUTPUT: (T, 1, X, Y, Z) → binary organ mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb7bcae-62d0-47f4-aba1-b0e42952cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (192, 192, 60, 176)\n",
      "Seg shape:   (192, 192, 60, 176)\n",
      "Torch input: torch.Size([176, 1, 192, 192, 60])\n",
      "Torch seg:   torch.Size([176, 1, 192, 192, 60])\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load nii files\n",
    "img = nib.load(\"input.nii\").get_fdata()     # shape: (X,Y,Z,T)\n",
    "seg = nib.load(\"output.nii\").get_fdata()    # shape: (X,Y,Z,T)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Seg shape:  \", seg.shape)\n",
    "\n",
    "# Normalize MRI for stability\n",
    "img = (img - np.mean(img)) / (np.std(img) + 1e-8)\n",
    "\n",
    "# Convert to torch\n",
    "img = torch.tensor(img, dtype=torch.float32)\n",
    "seg = torch.tensor(seg, dtype=torch.float32)\n",
    "\n",
    "# Add channel dimension → shape becomes (T, 1, X, Y, Z)\n",
    "img = img.permute(3,0,1,2).unsqueeze(1)\n",
    "seg = seg.permute(3,0,1,2).unsqueeze(1)\n",
    "\n",
    "print(\"Torch input:\", img.shape)\n",
    "print(\"Torch seg:  \", seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06742e20-9047-4883-94ab-8efe02db7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = DoubleConv(1, 32)\n",
    "        self.p1 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.d2 = DoubleConv(32, 64)\n",
    "        self.p2 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.d3 = DoubleConv(64, 128)\n",
    "\n",
    "        self.u2 = nn.ConvTranspose3d(128, 64, 2, stride=2)\n",
    "        self.db2 = DoubleConv(128, 64)\n",
    "\n",
    "        self.u1 = nn.ConvTranspose3d(64, 32, 2, stride=2)\n",
    "        self.db1 = DoubleConv(64, 32)\n",
    "\n",
    "        # final 1-channel prediction\n",
    "        self.final = nn.Conv3d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.d1(x)\n",
    "        p1 = self.p1(c1)\n",
    "\n",
    "        c2 = self.d2(p1)\n",
    "        p2 = self.p2(c2)\n",
    "\n",
    "        bottleneck = self.d3(p2)\n",
    "\n",
    "        u2 = self.u2(bottleneck)\n",
    "        merge2 = torch.cat([u2, c2], dim=1)\n",
    "        c2d = self.db2(merge2)\n",
    "\n",
    "        u1 = self.u1(c2d)\n",
    "        merge1 = torch.cat([u1, c1], dim=1)\n",
    "        c1d = self.db1(merge1)\n",
    "\n",
    "        # output = torch.sigmoid(self.final(c1d))\n",
    "        output = self.final(c1d)   # no sigmoid here\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd41fdb-359e-4f6d-b05c-82d264d104f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "dataset = TensorDataset(img, seg)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = UNet3D().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.0]).to(device))\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9891d4fc-9b21-48a9-b48d-49c4339a174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:40<00:00, 12.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss = 0.4111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:42<00:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss = 0.3323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss = 0.2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss = 0.2597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss = 0.2349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss = 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss = 0.1957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss = 0.1792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss = 0.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [17:41<00:00, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss = 0.1506\n",
      "✅ Model saved as unet_trained.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    \n",
    "    for batch_img, batch_seg in tqdm(loader):\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_seg = batch_seg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_img)\n",
    "        loss = loss_fn(pred, batch_seg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss = {running/len(loader):.4f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# SAVE TRAINED UNET MODEL\n",
    "# ------------------------------------------------\n",
    "torch.save(model.state_dict(), \"unet_trained.pth\")\n",
    "print(\"✅ Model saved as unet_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6009a5-9f2b-4292-b32e-8c7c88a71642",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(img.shape[0]):\n",
    "        X = img[t:t+1].to(device)\n",
    "        y = model(X)               # raw logits\n",
    "        y = torch.sigmoid(y)       # <-- ADD HERE\n",
    "        y = (y > 0.5).float()      # <-- AND HERE\n",
    "        preds.append(y.cpu())\n",
    "\n",
    "pred_4d = torch.cat(preds, dim=0)          # (T,1,X,Y,Z)\n",
    "pred_4d = pred_4d.squeeze(1).permute(1,2,3,0)  # (X,Y,Z,T)\n",
    "pred_np = pred_4d.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e8b9a3-f1d3-404b-be47-787d3e0e6e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predicted_segmentation.nii\n"
     ]
    }
   ],
   "source": [
    "nii = nib.Nifti1Image(pred_np, np.eye(4))\n",
    "nib.save(nii, \"predicted_segmentation.nii\")\n",
    "\n",
    "print(\"Saved predicted_segmentation.nii\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
